{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c15aec-5994-4168-bd71-eefdac291e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f536ac-d253-4215-84b3-04e4a5c6d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, hidden_d, output_d):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.LazyLinear(hidden_d)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(hidden_d, output_d)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.act(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed638758-e79e-4a00-9cb7-fbeae26d1e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0449,  0.3388, -0.2792,  0.0094, -0.0191,  0.1003,  0.0348, -0.1328],\n",
       "        [ 0.0449,  0.3388, -0.2792,  0.0094, -0.0191,  0.1003,  0.0348, -0.1328],\n",
       "        [ 0.0449,  0.3388, -0.2792,  0.0094, -0.0191,  0.1003,  0.0348, -0.1328],\n",
       "        [ 0.0449,  0.3388, -0.2792,  0.0094, -0.0191,  0.1003,  0.0348, -0.1328],\n",
       "        [ 0.0449,  0.3388, -0.2792,  0.0094, -0.0191,  0.1003,  0.0348, -0.1328]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = FFN(100, 8)\n",
    "test_tensor = torch.ones((5, 100))\n",
    "ffn(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a38c7deb-3a4d-49ba-9e51-5fbe8e400808",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = nn.LayerNorm(3)\n",
    "bn = nn.LazyBatchNorm1d()\n",
    "test_tensor = torch.tensor([[0, 2, 3], [2, 3, 4], [2, 3, 4], [5, 6, 7]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3315da80-3523-4e8f-9f01-93f8a4a5050f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3363,  0.2673,  1.0690],\n",
       "        [-1.2247,  0.0000,  1.2247],\n",
       "        [-1.2247,  0.0000,  1.2247],\n",
       "        [-1.2247,  0.0000,  1.2247]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4f93b72-5c79-4a95-ae2a-1fca29a598e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2603, -1.0000, -1.0000],\n",
       "        [-0.1400, -0.3333, -0.3333],\n",
       "        [-0.1400, -0.3333, -0.3333],\n",
       "        [ 1.5403,  1.6667,  1.6667]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd9e4f79-f1dd-4889-a836-afe5f983a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, norm_shape, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(norm_shape)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.norm(self.drop(Y) + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9462427-ab43-418c-a43a-d871a86d2710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [-0.7071, -0.7071,  1.4142]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an = AddNorm(3, 0.1)\n",
    "test_x = torch.ones((2, 3, 3))\n",
    "test_y = torch.ones((2, 3, 3))\n",
    "an(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cbf3aee-7a35-4100-80d7-c853583d8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, num_hidden, num_head, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.W_q = nn.LazyLinear(num_hidden, bias=use_bias)\n",
    "        self.W_k = nn.LazyLinear(num_hidden, bias=use_bias)\n",
    "        self.W_v = nn.LazyLinear(num_hidden, bias=use_bias)\n",
    "        self.W_o = nn.LazyLinear(num_hidden, bias=use_bias)\n",
    "\n",
    "    def transpose_qkv(self, x):\n",
    "        x = x.reshape(x.shape[0], x.shape[1], self.num_head, -1)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = x.reshape(-1, x.shape[2], x.shape[3])\n",
    "        return x\n",
    "\n",
    "    def transpose_o(self, x):\n",
    "        # x shape: (batch * num_head, seq_len, traits_dim / num_head)\n",
    "        x = x.reshape(-1, self.num_head, x.shape[1], x.shape[2])\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "        return x\n",
    "\n",
    "    def dot_product_attetion_score(self, q, k, v):\n",
    "        d = q.shape[-1]\n",
    "\n",
    "    def sequence_mask(self, valid_len, x, value=0):\n",
    "        #shape of valid_len: (batch)\n",
    "        #shape of x: (batch, dim)\n",
    "        max_len = x.shape[-1]\n",
    "        mask = torch.repeat_interleave((torch.arange(max_len, dtype=torch.float32)).reshape(1, -1), repeats=valid_len.shape[0], dim=0)\n",
    "        valid_len = torch.repeat_interleave(valid_len, repeats=max_len).reshape(-1, max_len)\n",
    "        mask = mask < valid_len\n",
    "        x[~mask] = value\n",
    "        return x\n",
    "\n",
    "    def masked_softmax(self, valid_len, x):\n",
    "        # shape of x: (batch, query_size, kv_size)\n",
    "        # it means, in each batch, there are query_size query and\n",
    "        # each of them map to kv_size key-value pair.\n",
    "        # shape of valid_len: (batch, query_size) or (batch)\n",
    "        # it means for each query, we should just take care how\n",
    "        # many key-value pair. So we could say each element in valid_len\n",
    "        # denotes the valid length of a batch\n",
    "        if valid_len is None:\n",
    "            return nn.functional.softmax(x, dim=-1)\n",
    "        else:\n",
    "            shape = x.shape\n",
    "            if valid_len.dim() == 1:\n",
    "                valid_len = torch.repeat_interleave(valid_len, shape[1])\n",
    "            else:\n",
    "                valid_len = valid_len.reshape(-1)\n",
    "\n",
    "            x = x.reshape((-1, shape[-1]))\n",
    "            x = self.sequence_mask(valid_len, x, value=-1e6)\n",
    "            return nn.functional.softmax(x.reshape(shape), dim=-1)\n",
    "\n",
    "    def forward(self, Q, K, V, valid_len=None):\n",
    "        # shape of Q, K and V: (Batch, seq_len, traits_dimension)\n",
    "        q = self.transpose_qkv(self.W_q(Q))\n",
    "        k = self.transpose_qkv(self.W_k(K))\n",
    "        v = self.transpose_qkv(self.W_v(V))\n",
    "        # shape of q, k and v: (Batch * num_head, seq_len, traits_dimension / num_head)\n",
    "        attetion_score = torch.bmm(q, k.transpose(1, 2)) / math.sqrt(q.shape[-1])\n",
    "        if valid_len is not None:\n",
    "            valid_len = torch.repeat_interleave(valid_len, repeats=self.num_head, dim=0)\n",
    "        self.attention_weights = self.masked_softmax(valid_len, attetion_score)\n",
    "        # res shape: (batch * num_head, seq_len, traits_dim / num_head)\n",
    "        res = torch.bmm(self.dropout(self.attention_weights), v)\n",
    "        # o shape: (batch, seq_len, traits_dim)\n",
    "        o = self.transpose_o(res)\n",
    "        output = self.W_o(o)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbe69fc6-6f23-4df3-9d1d-e28d74e3665f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
       "           0.0000, 0.0000],\n",
       "          [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
       "           0.0000, 0.0000],\n",
       "          [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
       "           0.0000, 0.0000],\n",
       "          [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
       "           0.0000, 0.0000],\n",
       "          [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
       "           0.0000, 0.0000]]]),\n",
       " torch.Size([2, 5, 10]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = MultiheadAttention(8, 2, 0)\n",
    "# query size is 5, kv size is 10.\n",
    "# query size is length of query, kv size is the length of kv sequence.\n",
    "test_mask = torch.ones((2, 5, 10))\n",
    "test_mask_valid_len = torch.tensor([6, 8])\n",
    "test_res = att.masked_softmax(test_mask_valid_len, test_mask)\n",
    "test_res, test_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a69381e3-6924-46ee-9e7f-13454aa8d88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = MultiheadAttention(8, 2, 0)\n",
    "test_att = torch.ones((3, 5, 8))\n",
    "res = att(test_att, test_att, test_att)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e43359e1-dcda-4abe-8874-14207eba58b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, ffn_hidden_d, num_head, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadAttention(hidden_d, num_head, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(hidden_d, dropout)\n",
    "        self.ffn = FFN(ffn_hidden_d, hidden_d)\n",
    "        self.addnorm2 = AddNorm(hidden_d, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens=None):\n",
    "        # X shape: (batch, seq_len, traits_dim)\n",
    "        attention_output = self.attention(X, X, X, valid_lens)\n",
    "        addnorm1 = self.addnorm1(X, attention_output)\n",
    "        ffn = self.ffn(addnorm1)\n",
    "        addnorm2 = self.addnorm2(addnorm1, ffn)\n",
    "        return addnorm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84f35172-24cf-49ff-a0ba-4d09f2734597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 1st dim for teb should equal to last dim for test_teb,\n",
    "# because we should it's additive for add norm layer 1.\n",
    "teb = TransformerEncoderBlock(8, 16, 2, 0)\n",
    "test_teb = torch.ones((3, 5, 8))\n",
    "res = teb(test_teb)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd510e4b-a70d-440e-b490-1bf274ec9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hidden):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hidden = num_hidden\n",
    "        self.linear = nn.Linear(vocab_size, num_hidden, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # shape of X: (batch, seq_len)\n",
    "        one_hot_x = torch.nn.functional.one_hot(X, num_classes=self.vocab_size).float()\n",
    "        embedding_res = self.linear(one_hot_x)\n",
    "        return embedding_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20bf95d2-7f60-4f29-8d3c-1056edfc1183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0136,  0.0778, -0.0492,  0.1628,  0.2445, -0.2048,  0.1555, -0.0560,\n",
       "           0.1219,  0.0179,  0.2353,  0.2344, -0.2532, -0.0658, -0.0751, -0.2879,\n",
       "           0.0574,  0.2323,  0.1044,  0.2377],\n",
       "         [-0.1416,  0.0018, -0.2485,  0.0349, -0.0452, -0.0141,  0.0453,  0.2215,\n",
       "           0.1966, -0.1201, -0.1656,  0.2511, -0.2178, -0.1673,  0.0027, -0.0467,\n",
       "           0.1079, -0.0530,  0.1784,  0.2047],\n",
       "         [-0.0287,  0.0944, -0.2321, -0.0761, -0.2841,  0.2770,  0.0496, -0.0037,\n",
       "          -0.1656, -0.2322, -0.1400, -0.1111,  0.0765, -0.0110,  0.2477,  0.1008,\n",
       "          -0.1553,  0.1353, -0.2510, -0.0174]], grad_fn=<MmBackward0>),\n",
       " torch.Size([3, 20]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emd = Embedding(10, 20)\n",
    "test_emd = torch.tensor([1, 5, 6])\n",
    "emd_res = emd(test_emd)\n",
    "emd_res, emd_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16b14919-29e2-4e24-bb6f-111386d07579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, num_dim, dropout, max_len=1000):\n",
    "        super().__init__()\n",
    "        # We use a fixed priori as P for current position encoding now.\n",
    "        # There are 3 dims:\n",
    "        # 1st dim for batch, because we could use boardcast mechanism, we set it as 1.\n",
    "        # 2nd dim is the max len, we think it's t he row index for calculation.\n",
    "        # 3rd dim is the num_dim, we think it's the column for calculation.\n",
    "        self.P = torch.empty((1, max_len, num_dim))\n",
    "        tmp = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(\n",
    "            1000, torch.arange(0, num_dim, 2, dtype=torch.float32) / num_dim)\n",
    "        self.P[:, :, 0::2] = torch.sin(tmp)\n",
    "        self.P[:, :, 1::2] = torch.cos(tmp)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        tmp = X + self.P[:, :X.shape[1],:].to(X.device)\n",
    "        return self.drop(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97a2ef43-ea09-428e-9101-c67d8196977e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 20])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionEncoding(20, 0, 10)\n",
    "pe_test = torch.randn((3, 8, 20))\n",
    "pe_res = pe(pe_test)\n",
    "pe_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5d63d8e-ef87-4515-9b62-5709b19c9abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hidden, ffn_num_hidden, num_head, num_blks, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.embedding = Embedding(vocab_size, num_hidden)\n",
    "        self.pos_encoding = PositionEncoding(num_hidden, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), TransformerEncoderBlock(num_hidden, ffn_num_hidden, num_head, dropout, use_bias))\n",
    "\n",
    "    # shape of X: (batch, seq_len)\n",
    "    def forward(self, X, valid_lens=None):\n",
    "        #shape of embedding: (batch, seq_len, num_hidden)\n",
    "        embedding = self.embedding(X)\n",
    "        #shape of pos_encoding: (batch, seq_len, num_hidden)\n",
    "        pos_encoding = self.pos_encoding(embedding * math.sqrt(self.num_hidden))\n",
    "        output = pos_encoding\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            output = blk(output, valid_lens)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef80bd82-47f8-4d03-8f33-889318f96a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4015,  0.6324, -0.3742, -0.5606,  0.1977,  1.1944, -0.6989,\n",
       "           -0.8887,  0.7093, -0.7248, -0.0934,  1.7166, -1.6133,  0.4738,\n",
       "            0.0736,  1.7724, -0.4961,  1.2448, -2.1177, -0.0457],\n",
       "          [ 0.3790,  0.9859,  0.8214, -0.3690, -0.6692,  1.0659, -1.7662,\n",
       "           -0.6512,  0.6832, -0.6441, -0.7891,  0.2895, -1.4407,  0.9283,\n",
       "           -1.3427,  1.2187, -0.2690,  0.4907, -0.9162,  1.9949],\n",
       "          [-0.6043, -1.5576,  0.7326,  0.0359, -0.1923,  1.0640,  0.6331,\n",
       "           -0.7142, -1.3970, -0.4047, -1.0121, -0.2890,  0.2993,  0.3074,\n",
       "            0.0421,  0.0322,  0.3062,  2.8072, -1.3098,  1.2208],\n",
       "          [ 0.6357, -0.9416,  1.4164, -1.3482,  0.5388, -0.8428, -0.3452,\n",
       "            0.7658, -0.1868,  0.5980, -0.7559,  0.1237,  0.4097,  1.1404,\n",
       "            0.0214,  1.7599, -1.5986,  0.3772, -2.1826,  0.4148],\n",
       "          [-1.3200, -0.8008,  0.3148, -1.9074, -0.3466,  1.5802,  0.7163,\n",
       "           -1.1239,  0.4652, -0.7073,  1.2105, -0.3560, -1.5596,  0.2025,\n",
       "            0.0293,  0.5659,  0.2002,  1.9973,  0.4054,  0.4342],\n",
       "          [-2.0693,  0.6354,  0.4850, -1.9668,  1.0571,  0.2678,  0.8240,\n",
       "            0.2658, -0.0785,  0.1860,  1.5293, -0.3503, -1.1259, -0.0094,\n",
       "           -1.5080, -0.7633,  0.0907,  1.4007,  0.4291,  0.7006],\n",
       "          [-0.2556,  0.5942,  0.7156, -1.1232,  0.8190, -0.8416, -0.5869,\n",
       "            1.1492, -1.0255, -0.6047,  0.9055,  0.9953, -1.9215,  0.9433,\n",
       "           -1.8459,  0.7618,  0.7168,  1.4160, -0.6488, -0.1630]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " torch.Size([1, 7, 20]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(10, 20, 30, 4, 2, 0)\n",
    "# shape of encoder_test: (1, 7), batch is 1, seq_len is 7\n",
    "encoder_test = torch.tensor([1, 2, 3, 4, 5, 6, 7]).reshape(1, -1)\n",
    "encoder_res = encoder(encoder_test)\n",
    "encoder_res, encoder_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8674c378-bacc-4328-bb0d-d59da1faa8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, num_hiddens, ffn_num_hiddens, num_head, dropout, i, use_bias=False):\n",
    "        super().__init__()\n",
    "        # i is the idx for this block\n",
    "        self.i = i\n",
    "        self.self_attention = MultiheadAttention(num_hiddens, num_head, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(num_hiddens, dropout)\n",
    "        self.cross_attention = MultiheadAttention(num_hiddens, num_head, dropout, use_bias)\n",
    "        self.addnorm2 = AddNorm(num_hiddens, dropout)\n",
    "        self.ffn = FFN(ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm3 = AddNorm(num_hiddens, dropout)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # shape of X: (batch_size, seq_len, num_hiddens)\n",
    "        # X came from decoder self.\n",
    "        # For example, at first step for decoder, it's bos (begin of sentence)\n",
    "        # state came from encoder, it's a list:\n",
    "        # [output from encoder, valid_len, [key and value for i-th decoder block]]\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        \n",
    "        self_key_value = X\n",
    "\n",
    "        # Given the argument of masked_softmax is attention score, its shape is\n",
    "        # (batch, query_size, kv_size), so the meanning of dec_len means how many\n",
    "        # kv the current query will pay attention to.\n",
    "        # Hence for the 1st query the dec_len should be 1, the 2nd query should be\n",
    "        # 2... For the last query, the dec_len should be qurry_total_len\n",
    "        batch_size, query_total_len = X.shape[0], X.shape[1]\n",
    "        dec_valid_len = torch.arange(1, query_total_len + 1, device=X.device).repeat(batch_size, 1)\n",
    "\n",
    "        self_att = self.self_attention(X, self_key_value, self_key_value, dec_valid_len)\n",
    "        addnor1 = self.addnorm1(X, self_att)\n",
    "        cross_att = self.cross_attention(addnor1, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "        addnor2 = self.addnorm1(addnor1, cross_att)\n",
    "        ffn = self.ffn(addnor2)\n",
    "        addnorm3 = self.addnorm3(addnor2, ffn)\n",
    "\n",
    "        return addnorm3, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20d43460-e068-4196-9983-a4d2f50cef70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_blk = TransformerDecoderBlock(10, 20, 2, 0, 0)\n",
    "test_dec_blk_x = torch.ones((2, 5, 10))\n",
    "test_dec_blk_state = [torch.randn((2, 7, 10)), torch.tensor([4, 6])]\n",
    "test_res = dec_blk(test_dec_blk_x, test_dec_blk_state)\n",
    "test_res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80fe062a-1c72-41b3-863f-e84bfd24c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_head, num_blks, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_blks = num_blks\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(self.num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), TransformerDecoderBlock(num_hiddens, ffn_num_hiddens, num_head, dropout, i))\n",
    "        self.linear = nn.LazyLinear(vocab_size)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # X shape is (batch, seq_len)\n",
    "        emb = self.embedding(X)\n",
    "        pos_encoding = self.pos_encoding(emb * math.sqrt(self.num_hiddens))\n",
    "        blk_input = pos_encoding\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            blk_input, state = blk(blk_input, state)\n",
    "        return self.linear(blk_input), state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "910548dd-ef67-40ba-96ae-ae048c732ef2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [2, 5] but got: [4, 5].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m test_td \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]])\n\u001b[1;32m      3\u001b[0m test_td_state \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m), torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])]\n\u001b[0;32m----> 4\u001b[0m test_res \u001b[38;5;241m=\u001b[39m \u001b[43mtd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_td\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_td_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m test_res[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[33], line 20\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, X, state)\u001b[0m\n\u001b[1;32m     18\u001b[0m blk_input \u001b[38;5;241m=\u001b[39m pos_encoding\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblks):\n\u001b[0;32m---> 20\u001b[0m     blk_input, state \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(blk_input), state\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[31], line 33\u001b[0m, in \u001b[0;36mTransformerDecoderBlock.forward\u001b[0;34m(self, X, state)\u001b[0m\n\u001b[1;32m     31\u001b[0m self_att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention(X, self_key_value, self_key_value, dec_valid_len)\n\u001b[1;32m     32\u001b[0m addnor1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddnorm1(X, self_att)\n\u001b[0;32m---> 33\u001b[0m cross_att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddnor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_valid_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m addnor2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddnorm1(addnor1, cross_att)\n\u001b[1;32m     35\u001b[0m ffn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(addnor2)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 64\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, Q, K, V, valid_len)\u001b[0m\n\u001b[1;32m     62\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_qkv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_v(V))\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# shape of q, k and v: (Batch * num_head, seq_len, traits_dimension / num_head)\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m attetion_score \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_len \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     valid_len \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(valid_len, repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_head, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [2, 5] but got: [4, 5]."
     ]
    }
   ],
   "source": [
    "td = TransformerDecoder(20, 10, 20, 2, 2, 0)\n",
    "test_td = torch.tensor([[1, 2]])\n",
    "test_td_state = [torch.randn(2, 5, 10), torch.tensor([2, 3])]\n",
    "test_res = td(test_td, test_td_state)\n",
    "test_res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904784e-e4db-491e-875c-da8d72e391a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b61d6-046b-466d-b7e1-065b891df418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cae963b-6517-4f0a-8568-482b006324df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "num_steps = 5\n",
    "dec_valid_lens = torch.arange(\n",
    "                1, num_steps + 1).repeat(batch_size, 1)\n",
    "dec_valid_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee8bb9cd-b2e7-4d93-81a3-ff2aeacb3dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<d2l.torch.MTFraEng object at 0x716eed3fd0c0>\n"
     ]
    }
   ],
   "source": [
    "data = d2l.MTFraEng(batch_size=128)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a962aa9b-98e4-4025-9a24-65831703514d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[158,  91,   2,  ...,   4,   4,   4],\n",
      "        [ 28, 122,   2,  ...,   4,   4,   4],\n",
      "        [183,  98,   2,  ...,   4,   4,   4],\n",
      "        ...,\n",
      "        [ 11, 163,   2,  ...,   4,   4,   4],\n",
      "        [ 39, 122,   2,  ...,   4,   4,   4],\n",
      "        [159,  91,   2,  ...,   4,   4,   4]]), tensor([[  3,   6,   0,  ...,   5,   5,   5],\n",
      "        [  3,  15,   0,  ...,   5,   5,   5],\n",
      "        [  3, 135,   6,  ...,   5,   5,   5],\n",
      "        ...,\n",
      "        [  3,   6,   2,  ...,   5,   5,   5],\n",
      "        [  3,   6,   0,  ...,   5,   5,   5],\n",
      "        [  3,   6,   2,  ...,   5,   5,   5]]), tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 5, 4, 3, 4, 4, 4, 4,\n",
      "        3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4,\n",
      "        4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4,\n",
      "        3, 4, 4, 4, 3, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4]), tensor([[  6,   0,   4,  ...,   5,   5,   5],\n",
      "        [ 15,   0,   4,  ...,   5,   5,   5],\n",
      "        [135,   6,   2,  ...,   5,   5,   5],\n",
      "        ...,\n",
      "        [  6,   2,   4,  ...,   5,   5,   5],\n",
      "        [  6,   0,   4,  ...,   5,   5,   5],\n",
      "        [  6,   2,   4,  ...,   5,   5,   5]])]\n",
      "torch.Size([128, 9])\n",
      "torch.Size([128, 9])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 9])\n"
     ]
    }
   ],
   "source": [
    "for a in data.train_dataloader():\n",
    "    print(a)\n",
    "    for tt in a:\n",
    "        print(tt.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f0d2c-7e29-41bc-a67d-2ea26216e81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
