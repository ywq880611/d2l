{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2c15aec-5994-4168-bd71-eefdac291e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8f536ac-d253-4215-84b3-04e4a5c6d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, hidden_d, output_d):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.LazyLinear(hidden_d)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(hidden_d, output_d)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.act(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed638758-e79e-4a00-9cb7-fbeae26d1e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0923, -0.1244, -0.0296,  0.2299, -0.0257, -0.3215,  0.0891, -0.0597],\n",
       "        [-0.0923, -0.1244, -0.0296,  0.2299, -0.0257, -0.3215,  0.0891, -0.0597],\n",
       "        [-0.0923, -0.1244, -0.0296,  0.2299, -0.0257, -0.3215,  0.0891, -0.0597],\n",
       "        [-0.0923, -0.1244, -0.0296,  0.2299, -0.0257, -0.3215,  0.0891, -0.0597],\n",
       "        [-0.0923, -0.1244, -0.0296,  0.2299, -0.0257, -0.3215,  0.0891, -0.0597]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = FFN(100, 8)\n",
    "test_tensor = torch.ones((5, 100))\n",
    "ffn(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a38c7deb-3a4d-49ba-9e51-5fbe8e400808",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = nn.LayerNorm(3)\n",
    "bn = nn.LazyBatchNorm1d()\n",
    "test_tensor = torch.tensor([[0, 2, 3], [2, 3, 4], [2, 3, 4], [5, 6, 7]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3315da80-3523-4e8f-9f01-93f8a4a5050f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3363,  0.2673,  1.0690],\n",
       "        [-1.2247,  0.0000,  1.2247],\n",
       "        [-1.2247,  0.0000,  1.2247],\n",
       "        [-1.2247,  0.0000,  1.2247]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4f93b72-5c79-4a95-ae2a-1fca29a598e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2603, -1.0000, -1.0000],\n",
       "        [-0.1400, -0.3333, -0.3333],\n",
       "        [-0.1400, -0.3333, -0.3333],\n",
       "        [ 1.5403,  1.6667,  1.6667]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd9e4f79-f1dd-4889-a836-afe5f983a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, norm_shape, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(norm_shape)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.norm(self.drop(Y) + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9462427-ab43-418c-a43a-d871a86d2710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7071, -1.4142,  0.7071],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-1.4142,  0.7071,  0.7071],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.7071, -1.4142,  0.7071]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an = AddNorm(3, 0.1)\n",
    "test_x = torch.ones((2, 3, 3))\n",
    "test_y = torch.ones((2, 3, 3))\n",
    "an(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9cbf3aee-7a35-4100-80d7-c853583d8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, num_hidden, num_head, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.W_q = nn.LazyLinear(num_hidden, bias=use_bias)\n",
    "        self.W_k = nn.LazyLinear(num_hidden, bias=use_bias)\n",
    "        self.W_v = nn.LazyLinear(num_hidden, bias=use_bias)\n",
    "        self.W_o = nn.LazyLinear(num_hidden, bias=use_bias)\n",
    "\n",
    "    def transpose_qkv(self, x):\n",
    "        x = x.reshape(x.shape[0], x.shape[1], self.num_head, -1)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = x.reshape(-1, x.shape[2], x.shape[3])\n",
    "        return x\n",
    "\n",
    "    def transpose_o(self, x):\n",
    "        # x shape: (batch * num_head, seq_len, traits_dim / num_head)\n",
    "        x = x.reshape(-1, self.num_head, x.shape[1], x.shape[2])\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "        return x\n",
    "\n",
    "    def dot_product_attetion_score(self, q, k, v):\n",
    "        d = q.shape[-1]\n",
    "\n",
    "    def sequence_mask(self, valid_len, x, value=0):\n",
    "        #shape of valid_len: (batch)\n",
    "        #shape of x: (batch, dim)\n",
    "        max_len = x.shape[-1]\n",
    "        mask = torch.repeat_interleave((torch.arange(max_len, dtype=torch.float32)).reshape(1, -1), repeats=valid_len.shape[0], dim=0)\n",
    "        valid_len = torch.repeat_interleave(valid_len, repeats=max_len).reshape(-1, max_len)\n",
    "        mask = mask < valid_len\n",
    "        x[~mask] = value\n",
    "        return x\n",
    "\n",
    "    def masked_softmax(self, valid_len, x):\n",
    "        # shape of x: (batch, seq_len, seq_len)\n",
    "        # it means, in each batch, there are seq_len query and\n",
    "        # each of them map to seq_len key-value pair.\n",
    "        # shape of valid_len: (batch, seq_len)\n",
    "        # it means for each query, we should just take care how\n",
    "        # many key-value pair.\n",
    "        if valid_len is None:\n",
    "            return nn.functional.softmax(x, dim=-1)\n",
    "        else:\n",
    "            shape = x.shape\n",
    "            if valid_len.dim() == 1:\n",
    "                valid_len = torch.repeat_interleave(valid_len, shape[1])\n",
    "            else:\n",
    "                valid_len = valid_len.reshape(-1)\n",
    "\n",
    "            x = x.reshape((-1, shape[-1]))\n",
    "            x = sequence_mask(valid_len, x, value=-1e6)\n",
    "            return nn.functional.softmax(x.reshape(shape), dim=-1)\n",
    "\n",
    "    def forward(self, Q, K, V, valid_len=None):\n",
    "        # shape of Q, K and V: (Batch, seq_len, traits_dimension)\n",
    "        q = self.transpose_qkv(self.W_q(Q))\n",
    "        k = self.transpose_qkv(self.W_k(K))\n",
    "        v = self.transpose_qkv(self.W_v(V))\n",
    "        # shape of q, k and v: (Batch * num_head, seq_len, traits_dimension / num_head)\n",
    "        attetion_score = torch.bmm(q, k.transpose(1, 2)) / math.sqrt(q.shape[-1])\n",
    "        if valid_len is not None:\n",
    "            valid_len = torch.repeat_interleave(valid_len, repeats=self.num_hidden, dim=0)\n",
    "        self.attention_weights = self.masked_softmax(valid_len, attetion_score)\n",
    "        # res shape: (batch * num_head, seq_len, traits_dim / num_head)\n",
    "        res = torch.bmm(self.dropout(self.attention_weights), v)\n",
    "        # o shape: (batch, seq_len, traits_dim)\n",
    "        o = self.transpose_o(res)\n",
    "        output = self.W_o(o)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a69381e3-6924-46ee-9e7f-13454aa8d88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 8])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = MultiheadAttention(8, 2, 0)\n",
    "test_att = torch.ones((3, 5, 8))\n",
    "res = att(test_att, test_att, test_att)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e43359e1-dcda-4abe-8874-14207eba58b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, ffn_hidden_d, num_head, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadAttention(hidden_d, num_head, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(hidden_d, dropout)\n",
    "        self.ffn = FFN(ffn_hidden_d, hidden_d)\n",
    "        self.addnorm2 = AddNorm(hidden_d, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens=None):\n",
    "        # X shape: (batch, seq_len, traits_dim)\n",
    "        attention_output = self.attention(X, X, X, valid_lens)\n",
    "        addnorm1 = self.addnorm1(X, attention_output)\n",
    "        ffn = self.ffn(addnorm1)\n",
    "        addnorm2 = self.addnorm2(addnorm1, ffn)\n",
    "        return addnorm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84f35172-24cf-49ff-a0ba-4d09f2734597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 8])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 1st dim for teb should equal to last dim for test_teb,\n",
    "# because we should it's additive for add norm layer 1.\n",
    "teb = TransformerEncoderBlock(8, 16, 2, 0)\n",
    "test_teb = torch.ones((3, 5, 8))\n",
    "res = teb(test_teb)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd510e4b-a70d-440e-b490-1bf274ec9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hidden):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hidden = num_hidden\n",
    "        self.linear = nn.Linear(vocab_size, num_hidden, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # shape of X: (batch, seq_len)\n",
    "        one_hot_x = torch.nn.functional.one_hot(X, num_classes=self.vocab_size).float()\n",
    "        embedding_res = self.linear(one_hot_x)\n",
    "        return embedding_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20bf95d2-7f60-4f29-8d3c-1056edfc1183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0356, -0.0287, -0.2012,  0.2273, -0.2930,  0.2950, -0.0294, -0.1253,\n",
       "           0.0435, -0.2875,  0.0802,  0.0469,  0.0069, -0.0489, -0.0305,  0.0602,\n",
       "           0.1467, -0.2299,  0.0860, -0.1267],\n",
       "         [-0.1032, -0.1029, -0.0809, -0.3148, -0.3031,  0.1387,  0.1939, -0.0663,\n",
       "          -0.2699, -0.0253, -0.1051, -0.0866, -0.1352,  0.2682,  0.0427, -0.1792,\n",
       "           0.2152, -0.1289, -0.1675, -0.1323],\n",
       "         [ 0.0229,  0.1520,  0.1598,  0.0492, -0.2183,  0.0232, -0.2028, -0.2805,\n",
       "          -0.2325,  0.1253,  0.2482,  0.0285, -0.0703,  0.2733, -0.1834, -0.1215,\n",
       "          -0.0719, -0.2733, -0.1391, -0.0980]], grad_fn=<MmBackward0>),\n",
       " torch.Size([3, 20]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emd = Embedding(10, 20)\n",
    "test_emd = torch.tensor([1, 5, 6])\n",
    "emd_res = emd(test_emd)\n",
    "emd_res, emd_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "16b14919-29e2-4e24-bb6f-111386d07579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, num_dim, dropout, max_len=1000):\n",
    "        super().__init__()\n",
    "        # We use a fixed priori as P for current position encoding now.\n",
    "        # There are 3 dims:\n",
    "        # 1st dim for batch, because we could use boardcast mechanism, we set it as 1.\n",
    "        # 2nd dim is the max len, we think it's t he row index for calculation.\n",
    "        # 3rd dim is the num_dim, we think it's the column for calculation.\n",
    "        self.P = torch.empty((1, max_len, num_dim))\n",
    "        tmp = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(\n",
    "            1000, torch.arange(0, num_dim, 2, dtype=torch.float32) / num_dim)\n",
    "        self.P[:, :, 0::2] = torch.sin(tmp)\n",
    "        self.P[:, :, 1::2] = torch.cos(tmp)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        tmp = X + self.P[:, :X.shape[1],:].to(X.device)\n",
    "        return self.drop(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97a2ef43-ea09-428e-9101-c67d8196977e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 20])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding(20, 0, 10)\n",
    "pe_test = torch.randn((3, 8, 20))\n",
    "pe_res = pe(pe_test)\n",
    "pe_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5d63d8e-ef87-4515-9b62-5709b19c9abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hidden, ffn_num_hidden, num_head, num_blks, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.embedding = Embedding(vocab_size, num_hidden)\n",
    "        self.pos_encoding = PositionEncoding(num_hidden, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), TransformerEncoderBlock(num_hidden, ffn_num_hidden, num_head, dropout, use_bias))\n",
    "\n",
    "    # shape of X: (batch, seq_len)\n",
    "    def forward(self, X, valid_lens=None):\n",
    "        #shape of embedding: (batch, seq_len, num_hidden)\n",
    "        embedding = self.embedding(X)\n",
    "        #shape of pos_encoding: (batch, seq_len, num_hidden)\n",
    "        pos_encoding = self.pos_encoding(embedding * math.sqrt(self.num_hidden))\n",
    "        output = pos_encoding\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            output = blk(output, valid_lens)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef80bd82-47f8-4d03-8f33-889318f96a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-4.4308e-01,  1.3189e+00, -8.5987e-01,  2.0115e+00, -1.4273e+00,\n",
       "            3.4615e-02, -7.2143e-01, -4.8976e-01,  3.3794e-01, -3.7318e-01,\n",
       "           -1.4779e+00,  5.2810e-01, -7.9528e-01,  1.5515e+00, -5.3594e-01,\n",
       "           -1.1595e+00,  9.0920e-04,  9.5331e-01,  7.0481e-02,  1.4759e+00],\n",
       "          [-3.8693e-01,  8.2518e-01, -1.5203e+00,  2.3357e-01,  2.6728e-02,\n",
       "            9.4459e-01,  4.7949e-02,  1.2375e+00,  1.0800e+00,  1.5880e+00,\n",
       "           -8.6881e-01,  1.5499e+00, -1.0151e+00,  6.0419e-01, -1.6954e+00,\n",
       "            2.7596e-01, -7.5043e-01,  1.0690e-01, -1.1851e+00, -1.0983e+00],\n",
       "          [ 8.2149e-01, -2.4622e+00,  7.3896e-01, -6.6537e-01,  8.6555e-01,\n",
       "           -5.5550e-01, -4.0124e-01, -6.2197e-01,  1.1920e+00,  1.0700e+00,\n",
       "            5.6262e-01,  8.3060e-01, -9.1356e-01,  1.8843e-01, -3.8616e-01,\n",
       "           -1.4513e+00, -6.9863e-01, -5.9390e-01,  1.0762e+00,  1.4040e+00],\n",
       "          [ 1.0509e-01, -1.4472e+00,  8.6075e-01,  6.3476e-01,  9.6304e-01,\n",
       "            3.5611e-01,  9.2762e-01, -1.1460e+00, -6.3270e-01, -2.2722e-01,\n",
       "            7.1849e-01, -1.4458e-01, -4.6784e-03,  2.0786e+00, -1.2372e-01,\n",
       "           -2.1758e+00,  1.0571e+00, -1.5176e+00, -1.8174e-01, -1.0040e-01],\n",
       "          [-3.2728e+00, -1.5583e+00, -6.1328e-01,  5.5119e-01,  6.6090e-01,\n",
       "            9.4281e-01,  1.0728e-01,  6.3881e-01,  2.2700e-01,  2.1945e-01,\n",
       "            1.1182e+00,  9.9009e-01, -6.9526e-01,  6.3251e-01,  6.1834e-02,\n",
       "           -3.9047e-01,  7.5780e-01,  3.4111e-01, -6.0020e-01, -1.1865e-01],\n",
       "          [-1.3517e+00, -5.0468e-01, -1.9056e-01, -1.3447e+00,  7.6183e-01,\n",
       "           -9.0221e-01,  5.7951e-01, -5.4865e-01,  1.1864e+00,  1.4610e+00,\n",
       "           -1.5575e+00,  4.5606e-01,  3.0361e-01,  1.8922e+00, -7.4511e-01,\n",
       "            1.1122e+00,  1.0907e+00, -5.6003e-01, -9.2307e-01, -2.1534e-01],\n",
       "          [-1.0128e+00,  1.9091e+00,  1.1725e+00, -9.4596e-02,  1.4158e+00,\n",
       "           -1.8583e-01, -4.2199e-01, -3.1548e-01, -6.6098e-01, -3.0936e-01,\n",
       "           -9.7845e-01,  1.7943e-02, -4.9253e-01,  1.7075e+00, -1.5098e+00,\n",
       "            6.7681e-01,  4.1956e-01,  3.8964e-01, -1.9645e+00,  2.3749e-01]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " torch.Size([1, 7, 20]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(10, 20, 30, 4, 2, 0)\n",
    "# shape of encoder_test: (1, 7), batch is 1, seq_len is 7\n",
    "encoder_test = torch.tensor([1, 2, 3, 4, 5, 6, 7]).reshape(1, -1)\n",
    "encoder_res = encoder(encoder_test)\n",
    "encoder_res, encoder_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8674c378-bacc-4328-bb0d-d59da1faa8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
